{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDoing it for post processed stuff\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Doing it for post processed stuff\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/blue/kgong/s.kapoor/language_guided_segmentation/CLIP-Driven-Universal-Model\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import nibabel as nib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gg_tools import get_train_val_data_loader, get_train_val_txt_loader, dice_score, TEMPLATE, get_key_2, NUM_CLASS, ORGAN_NAME, organ_post_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score_2(preds, labels, spe_sen=False):  # on CPU with NumPy\n",
    "    ### preds: w,h,d; labels: w,h,d\n",
    "    assert preds.shape == labels.shape, \"predict & target batch size don't match\"\n",
    "    \n",
    "    # Flattening the arrays\n",
    "    predict = preds.ravel()\n",
    "    target = labels.ravel()\n",
    "\n",
    "    # True positives\n",
    "    tp = np.sum(predict * target)\n",
    "\n",
    "    # Denominator: sum of predicted and target pixels + 1 to avoid division by zero\n",
    "    den = np.sum(predict) + np.sum(target) + 1\n",
    "\n",
    "    # Dice score calculation\n",
    "    dice = 2 * tp / den\n",
    "\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now go through val_loader\n",
    "def validation_postprocess(model,val_loader,args):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    dice_list = {key: torch.zeros(2,NUM_CLASS).to(args.device) for key in TEMPLATE.keys()}\n",
    "\n",
    "    for batch in tqdm(val_loader):\n",
    "\n",
    "        \n",
    "        if(args.model_type == 'film'):\n",
    "            image, label, name, prompt = batch['image'].to(args.device), batch['post_label'], batch['name'], batch['prompt']\n",
    "        else:\n",
    "            image, label, name = batch['image'].to(args.device), batch['post_label'], batch['name']\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if(args.model_type == 'film'):\n",
    "                predictor = lambda image_patch:model(image_patch,prompt)\n",
    "                pred = sliding_window_inference(image, (args.roi_x, args.roi_y, args.roi_z), 1, predictor)\n",
    "            else:\n",
    "                pred = sliding_window_inference(image, (args.roi_x, args.roi_y, args.roi_z), 1, model)\n",
    "            pred_sigmoid = torch.nn.functional.sigmoid(pred)\n",
    "        \n",
    "        template_key = get_key_2(name[0]) #since for val_loader we have just 1 .\n",
    "        organ_list = TEMPLATE[template_key]\n",
    "\n",
    "        pred_sigmoid = torch.squeeze(pred_sigmoid)\n",
    "        pred_sigmoid = torch.where(pred_sigmoid>0.5,1.,0.)\n",
    "        pred_mask = pred_sigmoid.cpu().numpy()\n",
    "        post_processed_mask = organ_post_process(pred_mask,organ_list)\n",
    "        label = np.array(label)\n",
    "        label = np.squeeze(label)\n",
    "\n",
    "        for organ in organ_list:\n",
    "            dice_organ = dice_score_2(post_processed_mask[organ-1,:,:,:], label[organ-1,:,:,:])\n",
    "            dice_list[template_key][0][organ-1] += dice_organ\n",
    "            dice_list[template_key][1][organ-1] += 1\n",
    "    \n",
    "    avg_organ_dice = np.zeros((2,NUM_CLASS))\n",
    "\n",
    "    with open(args.file_name, 'w') as f:\n",
    "        for key in TEMPLATE.keys():\n",
    "            organ_list = TEMPLATE[key]\n",
    "            content = 'Task%s| '%(key)\n",
    "            for organ in organ_list:\n",
    "                dice = dice_list[key][0][organ-1] / dice_list[key][1][organ-1]\n",
    "                content += '%s: %.4f, '%(ORGAN_NAME[organ-1], dice)\n",
    "                avg_organ_dice[0][organ-1] += dice_list[key][0][organ-1]\n",
    "                avg_organ_dice[1][organ-1] += dice_list[key][1][organ-1]\n",
    "            f.write(content)\n",
    "            f.write('\\n')\n",
    "        content = 'Average | '\n",
    "        for i in range(NUM_CLASS):\n",
    "            content += '%s: %.4f, '%(ORGAN_NAME[i], avg_organ_dice[0][i] / avg_organ_dice[1][i])\n",
    "        f.write(content)\n",
    "        f.write('\\n')\n",
    "\n",
    "    return avg_organ_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_clip = SimpleNamespace(\n",
    "    space_x = 1.5,\n",
    "    space_y = 1.5,\n",
    "    space_z = 1.5,\n",
    "    roi_x = 96,\n",
    "    roi_y = 96,\n",
    "    roi_z = 96,\n",
    "    num_samples = 2,\n",
    "    data_root_path = '/blue/kgong/s.kapoor/language_guided_segmentation/CLIP-Driven-Universal-Model/',\n",
    "    data_txt_path = './dataset/dataset_list/',\n",
    "    batch_size = 4,\n",
    "    num_workers = 8,\n",
    "    a_min = -175,\n",
    "    a_max = 250,\n",
    "    b_min = 0.0,\n",
    "    b_max = 1.0,\n",
    "    dataset_list = ['PAOTtest'], #here it is used to vaidate the model\n",
    "    NUM_CLASS = NUM_CLASS,\n",
    "    backbone = 'swinunetr',\n",
    "    trans_encoding = 'word_embedding',\n",
    "    pretrain = './out/universal_total_org/epoch_400.pth',\n",
    "    lr = 4e-4,\n",
    "    weight_decay = 1e-5,\n",
    "    precomputed_prompt_path = './pretrained_weights/embeddings_template.pkl',\n",
    "    word_embedding = './pretrained_weights/txt_encoding.pth',\n",
    "    dist = False,\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    model_type = None,\n",
    "    file_name = 'paot_test_universal_postprocess.txt',\n",
    "    os_save_fold = './not_required'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Universal_model import Universal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the model\n",
    "clip_model = Universal_model(img_size=(args_clip.roi_x, args_clip.roi_y, args_clip.roi_z),\n",
    "                in_channels=1,\n",
    "                out_channels=32,\n",
    "                backbone=args_clip.backbone,\n",
    "                encoding=args_clip.trans_encoding\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_checkpoint = torch.load(args_clip.pretrain)\n",
    "store_dict = clip_model.state_dict()\n",
    "load_dict = clip_checkpoint['net']\n",
    "\n",
    "for key,value in load_dict.items():\n",
    "\n",
    "    if 'swinViT' in key or 'encoder' in key or 'decoder' in key:\n",
    "        name = '.'.join(key.split('.')[1:])\n",
    "    else:\n",
    "        name = '.'.join(key.split('.')[1:])\n",
    "    if name in store_dict.keys():\n",
    "        store_dict[name]=value\n",
    "    else:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model.load_state_dict(store_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = clip_model.to(args_clip.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len 583\n",
      "val len 583\n"
     ]
    }
   ],
   "source": [
    "clip_train_loader, clip_val_loader,clip_train_sampler, clip_val_sampler = get_train_val_data_loader(args_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 583/583 [1:06:30<00:00,  6.85s/it]\n"
     ]
    }
   ],
   "source": [
    "universal_dice, universal_dice_list = validation_postprocess(clip_model,clip_val_loader,args_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.SwinUNETR_DEEP_FILM import SwinUNETR_DEEP_FILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_film = SimpleNamespace(\n",
    "    space_x = 1.5,\n",
    "    space_y = 1.5,\n",
    "    space_z = 1.5,\n",
    "    roi_x = 96,\n",
    "    roi_y = 96,\n",
    "    roi_z = 96,\n",
    "    num_samples = 2,\n",
    "    data_root_path = '/blue/kgong/s.kapoor/language_guided_segmentation/CLIP-Driven-Universal-Model/',\n",
    "    data_txt_path = './dataset/dataset_list/',\n",
    "    batch_size = 4,\n",
    "    num_workers = 8,\n",
    "    a_min = -175,\n",
    "    a_max = 250,\n",
    "    b_min = 0.0,\n",
    "    b_max = 1.0,\n",
    "    dataset_list = ['PAOTtest'],\n",
    "    NUM_CLASS = NUM_CLASS,\n",
    "    backbone = 'swinunetr',\n",
    "    trans_encoding = 'word_embedding',\n",
    "    pretrain = './out/deep_film_org_setting/epoch_190.pth',\n",
    "    lr = 4e-4,\n",
    "    weight_decay = 1e-5,\n",
    "    precomputed_prompt_path = 'embeddings_template_flare.pkl',\n",
    "    dist = False,\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    model_type='film',\n",
    "    file_name='paot_test_film_org_setting_postporcess.txt',\n",
    "    os_save_fold = './flaretrain/deep_film'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_model = SwinUNETR_DEEP_FILM(img_size=(args_film.roi_x, args_film.roi_y, args_film.roi_z),\n",
    "                        in_channels=1,\n",
    "                        out_channels=32,\n",
    "                        precomputed_prompt_path=args_film.precomputed_prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_checkpoint = torch.load(args_film.pretrain)\n",
    "store_dict = film_model.state_dict()\n",
    "load_dict = film_checkpoint['net']\n",
    "\n",
    "for key,value in load_dict.items():\n",
    "\n",
    "    if 'swinViT' in key or 'encoder' in key or 'decoder' in key:\n",
    "        name = '.'.join(key.split('.')[1:])\n",
    "    else:\n",
    "        name = '.'.join(key.split('.')[1:])\n",
    "    if name in store_dict.keys():\n",
    "        store_dict[name]=value\n",
    "    else:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film_model.load_state_dict(store_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_model = film_model.to(args_film.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len 583\n",
      "val len 583\n"
     ]
    }
   ],
   "source": [
    "film_train_loader, film_val_loader,film_train_sampler, film_val_sampler = get_train_val_txt_loader(args_film)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                   | 0/583 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 583/583 [57:52<00:00,  5.96s/it]\n"
     ]
    }
   ],
   "source": [
    "film_avg_organ_dice, film_dice_list = validation_postprocess(film_model,film_val_loader,args_film)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
